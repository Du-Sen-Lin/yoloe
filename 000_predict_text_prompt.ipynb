{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7fde70ed-4167-427d-8373-6ba2ee98b91e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload \n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "927508c3-3aa3-4787-9e44-dbafc3d10cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "af7b361d-1982-4282-9e6f-a8d901d0843c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\" \n",
    "\n",
    "# 设置 HTTP 和 HTTPS 代理\n",
    "os.environ[\"http_proxy\"] = \"***\"\n",
    "os.environ[\"https_proxy\"] = \"***\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6fce82-a585-4ba4-9fe0-db0ad09761bf",
   "metadata": {},
   "source": [
    "# predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a4aa39-76ce-42d1-8a56-f4e8d38cd92b",
   "metadata": {},
   "source": [
    "## text prompt\n",
    "CUDA_VISIBLE_DEVICES=3 python predict_text_prompt.py \\\n",
    "    --source ultralytics/assets/bus.jpg \\\n",
    "    --checkpoint pretrain/yoloe-v8l-seg.pt \\\n",
    "    --names person dog cat \\\n",
    "    --device cuda:0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c671ca14-4974-40fa-b056-f55ce49ce170",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "from PIL import Image\n",
    "import supervision as sv\n",
    "from ultralytics import YOLOE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "365129c5-1e29-44c9-9790-31b8b8fcc881",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c62fef73-b978-43e0-a252-1fa7dd8f6c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "source = \"/root/project/research/Yolo/yoloe/test_data/000000039769.jpg\"\n",
    "checkpoint = \"./pretrain/yoloe-v8l-seg.pt\"\n",
    "\n",
    "# 微调\n",
    "# source = \"/root/dataset/glass_data_20250317/images/train/Image_20250210134520105.bmp\"\n",
    "# checkpoint = \"/root/project/research/Yolo/ultralytics/runs/segment/train6/weights/best.pt\"\n",
    "\n",
    "device = \"cuda:0\"\n",
    "# text_prompt = [\"person\", \"dog\", \"cat\"]\n",
    "# text_prompt = [\"remote control\"]\n",
    "# text_prompt = [\"一只躺在图片左边的猫\"]\n",
    "# text_prompt = [\"白色的遥控器\"]\n",
    "# text_prompt = [\"person\", \"dog\", \"two cats\", \"cat\", \"remote control\", \"sofa\", \"chair\", \"teacup\", \"coffee\"]  # 中文效果很差，label需要使用中文; 语义理解能力很差\n",
    "# text_prompt = [\"person\", \"dog\", \"two cats\", \"the cat on the far left\", \"remote control\", \"sofa\"]\n",
    "# text_prompt = [\"person\", \"dog\", \"two cats\", \"remote control\", \"sofa\"]\n",
    "# text_prompt = [\"coffee cup\"]\n",
    "\n",
    "text_prompt = [\"person\", \"dog\", \"two cats\", \"cat\", \"remote control\", \"sofa\", \"chair\", \"teacup\", \"coffee\", \"sliding sleeve\"] \n",
    "\n",
    "output = \"/root/project/research/Yolo/yoloe/test_data_out/\"\n",
    "os.makedirs(output, exist_ok=True)\n",
    "\n",
    "image = Image.open(source).convert(\"RGB\")\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(8, 8))\n",
    "# ax.imshow(image, cmap=\"gray\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "865caca4-cbd6-4a0d-8d78-a9da1447e8af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build text model mobileclip:blt\n",
      " model.get_text_pe(text_prompt): torch.Size([1, 10, 512])\n",
      "Build text model mobileclip:blt\n",
      "\n",
      "0: 640x640 2 cats, 2 remote controls, 1 sofa, 9.9ms\n",
      "Speed: 1.2ms preprocess, 9.9ms inference, 12.6ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "model = YOLOE(checkpoint)\n",
    "model.to(device)\n",
    "print(f\" model.get_text_pe(text_prompt): { model.get_text_pe(text_prompt).shape}\")\n",
    "model.set_classes(text_prompt, model.get_text_pe(text_prompt)) # model.get_text_pe(text_prompt).shape, torch.Size([1, x, 512])\n",
    "# results = model.predict(image, conf=0.25, verbose=True)\n",
    "results = model.predict(image, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "473a5084-be0f-4a6a-a3bb-abb8903e0a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annotated_image: (640, 480)\n"
     ]
    }
   ],
   "source": [
    "detections = sv.Detections.from_ultralytics(results[0])\n",
    "# print(f\"detections: {detections}\")\n",
    "\n",
    "resolution_wh = image.size\n",
    "\n",
    "thickness = sv.calculate_optimal_line_thickness(resolution_wh=resolution_wh)\n",
    "text_scale = sv.calculate_optimal_text_scale(resolution_wh=resolution_wh)\n",
    "\n",
    "labels = [\n",
    "    f\"{class_name} {confidence:.2f}\"\n",
    "    for class_name, confidence in zip(detections[\"class_name\"], detections.confidence)\n",
    "]\n",
    "\n",
    "annotated_image = image.copy()\n",
    "\n",
    "annotated_image = sv.MaskAnnotator(\n",
    "    color_lookup=sv.ColorLookup.INDEX,\n",
    "    opacity=0.4\n",
    ").annotate(scene=annotated_image, detections=detections)\n",
    "\n",
    "annotated_image = sv.BoxAnnotator(\n",
    "    color_lookup=sv.ColorLookup.INDEX,\n",
    "    thickness=thickness\n",
    ").annotate(scene=annotated_image, detections=detections)\n",
    "\n",
    "annotated_image = sv.LabelAnnotator(\n",
    "    color_lookup=sv.ColorLookup.INDEX,\n",
    "    text_scale=text_scale,\n",
    "    smart_position=True\n",
    ").annotate(scene=annotated_image, detections=detections, labels=labels)\n",
    "\n",
    "print(f\"annotated_image: {annotated_image.size}\")\n",
    "# fig, ax = plt.subplots(figsize=(8, 8))\n",
    "# ax.imshow(annotated_image, cmap=\"gray\")\n",
    "\n",
    "base, ext = os.path.splitext(source)\n",
    "path, filename = os.path.split(source)\n",
    "# print(f\"base: {path}, ext: {filename}\")\n",
    "output_name = os.path.join(output, \"re_\" + filename)\n",
    "# print(f\"output_name: {output_name}\")\n",
    "annotated_image.save(output_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c173fda-f731-48d7-85a7-b818674f79f6",
   "metadata": {},
   "source": [
    "## train pe result\n",
    "\n",
    "`text_prompt = ['bracket component', 'sliding sleeve', 'elastic bridge', 'fastening screw', 'fastening nut', 'small adjustment block', 'large adjustment block']` 数量与微调的 数据类别必须一致，否则会报错，相当于回到闭集\n",
    "\n",
    "\n",
    "```python\n",
    "from ultralytics import YOLOE\n",
    "\n",
    "# 加载训练好的模型\n",
    "import torch\n",
    "pe_data = torch.load(\"custom-pe.pt\")\n",
    "names = pe_data[\"names\"]\n",
    "print(f\"names: {names}\")\n",
    "pe = pe_data[\"pe\"]\n",
    "print(f\"pe: {pe.shape}\")\n",
    "model = YOLOE(\"/root/project/research/Yolo/ultralytics/runs/segment/train6/weights/last.pt\")\n",
    "model.set_classes(names, model.get_text_pe(names))\n",
    "\n",
    "# model = YOLOE(\"/root/project/research/Yolo/ultralytics/runs/segment/train6/weights/best.pt\")  # 或你自定义保存的路径\n",
    "\n",
    "# 推理图片\n",
    "results = model.predict(source='/root/dataset/glass_data_20250317/images/train/Image_20250210134520105.bmp', save=True)\n",
    "\n",
    "# 打印结果\n",
    "for result in results:\n",
    "    print(result.boxes)       # 检测框\n",
    "    print(result.masks)       # 分割掩膜（如果是 segment 模型）\n",
    "    print(result.probs)       # 类别概率（可选）\n",
    "    \n",
    "```\n",
    "or\n",
    "\n",
    "```python\n",
    "from ultralytics import YOLOE\n",
    "\n",
    "# 加载训练好的模型\n",
    "import torch\n",
    "model = YOLOE(\"/root/project/research/Yolo/ultralytics/runs/segment/train6/weights/best.pt\")  # 或你自定义保存的路径\n",
    "# 推理图片\n",
    "results = model.predict(source='/root/dataset/glass_data_20250317/images/train/Image_20250210134520105.bmp', save=True)\n",
    "\n",
    "# 打印结果\n",
    "for result in results:\n",
    "    print(result.boxes)       # 检测框\n",
    "    print(result.masks)       # 分割掩膜（如果是 segment 模型）\n",
    "    print(result.probs)       # 类别概率（可选）\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fb363975-85f2-44ec-882c-8d358a7d3c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "from PIL import Image\n",
    "import supervision as sv\n",
    "from ultralytics import YOLOE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c1243f4c-646b-408a-96c4-952a90ad6b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "edba63b9-2bf4-47cb-958f-c7a0dfb8b7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source = \"/root/project/research/Yolo/yoloe/test_data/000000039769.jpg\"\n",
    "# checkpoint = \"./pretrain/yoloe-v8l-seg.pt\"\n",
    "\n",
    "# 微调\n",
    "source = \"/root/dataset/glass_data_20250317/images/train/Image_20250210134520105.bmp\"\n",
    "checkpoint = \"/root/project/research/Yolo/ultralytics/runs/segment/train7/weights/best.pt\"\n",
    "\n",
    "device = \"cuda:0\"\n",
    "# text_prompt = [\"person\", \"dog\", \"cat\"]\n",
    "# text_prompt = [\"remote control\"]\n",
    "# text_prompt = [\"一只躺在图片左边的猫\"]\n",
    "# text_prompt = [\"白色的遥控器\"]\n",
    "# text_prompt = [\"person\", \"dog\", \"two cats\", \"cat\", \"remote control\", \"sofa\", \"chair\", \"teacup\", \"coffee\"]  # 中文效果很差，label需要使用中文; 语义理解能力很差\n",
    "# text_prompt = [\"person\", \"dog\", \"two cats\", \"the cat on the far left\", \"remote control\", \"sofa\"]\n",
    "# text_prompt = [\"person\", \"dog\", \"two cats\", \"remote control\", \"sofa\"]\n",
    "# text_prompt = [\"coffee cup\"]\n",
    "\n",
    "text_prompt = ['bracket component', 'sliding sleeve', 'elastic bridge', 'fastening screw', 'fastening nut', 'small adjustment block', 'large adjustment block']\n",
    "\n",
    "# text_prompt = ['bracket component', 'small adjustment block', 'large adjustment block']\n",
    "\n",
    "output = \"/root/project/research/Yolo/yoloe/test_data_out/\"\n",
    "os.makedirs(output, exist_ok=True)\n",
    "\n",
    "image = Image.open(source).convert(\"RGB\")\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(8, 8))\n",
    "# ax.imshow(image, cmap=\"gray\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "807b0ba6-d594-46f4-bd30-31e82885a094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build text model mobileclip:blt\n",
      " model.get_text_pe(text_prompt): torch.Size([1, 7, 512])\n",
      "Build text model mobileclip:blt\n",
      "\n",
      "0: 640x640 2 bracket components, 2 sliding sleeves, 2 elastic bridges, 12.0ms\n",
      "Speed: 2.3ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "model = YOLOE(checkpoint)\n",
    "model.to(device)\n",
    "print(f\" model.get_text_pe(text_prompt): { model.get_text_pe(text_prompt).shape}\")\n",
    "model.set_classes(text_prompt, model.get_text_pe(text_prompt)) # model.get_text_pe(text_prompt).shape, torch.Size([1, x, 512])\n",
    "# results = model.predict(image, conf=0.25, verbose=True)\n",
    "results = model.predict(image, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fafb51a0-eacd-4586-84c4-1b4f8603721f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annotated_image: (3072, 2048)\n",
      "base: /root/dataset/glass_data_20250317/images/train, ext: Image_20250210134520105.bmp\n",
      "output_name: /root/project/research/Yolo/yoloe/test_data_out/re_Image_20250210134520105.bmp\n"
     ]
    }
   ],
   "source": [
    "detections = sv.Detections.from_ultralytics(results[0])\n",
    "# print(f\"detections: {detections}\")\n",
    "\n",
    "resolution_wh = image.size\n",
    "\n",
    "thickness = sv.calculate_optimal_line_thickness(resolution_wh=resolution_wh)\n",
    "text_scale = sv.calculate_optimal_text_scale(resolution_wh=resolution_wh)\n",
    "\n",
    "labels = [\n",
    "    f\"{class_name} {confidence:.2f}\"\n",
    "    for class_name, confidence in zip(detections[\"class_name\"], detections.confidence)\n",
    "]\n",
    "\n",
    "annotated_image = image.copy()\n",
    "\n",
    "annotated_image = sv.MaskAnnotator(\n",
    "    color_lookup=sv.ColorLookup.INDEX,\n",
    "    opacity=0.4\n",
    ").annotate(scene=annotated_image, detections=detections)\n",
    "\n",
    "annotated_image = sv.BoxAnnotator(\n",
    "    color_lookup=sv.ColorLookup.INDEX,\n",
    "    thickness=thickness\n",
    ").annotate(scene=annotated_image, detections=detections)\n",
    "\n",
    "annotated_image = sv.LabelAnnotator(\n",
    "    color_lookup=sv.ColorLookup.INDEX,\n",
    "    text_scale=text_scale,\n",
    "    smart_position=True\n",
    ").annotate(scene=annotated_image, detections=detections, labels=labels)\n",
    "\n",
    "print(f\"annotated_image: {annotated_image.size}\")\n",
    "# fig, ax = plt.subplots(figsize=(8, 8))\n",
    "# ax.imshow(annotated_image, cmap=\"gray\")\n",
    "\n",
    "base, ext = os.path.splitext(source)\n",
    "path, filename = os.path.split(source)\n",
    "print(f\"base: {path}, ext: {filename}\")\n",
    "output_name = os.path.join(output, \"re_\" + filename)\n",
    "print(f\"output_name: {output_name}\")\n",
    "annotated_image.save(output_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f84fa8-d530-46ac-9d07-42815c6ba46a",
   "metadata": {},
   "source": [
    "## COCO 转 labelme json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "56823c25-66f4-46d4-b49d-c942b108e248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "转换完成，所有文件已保存到：/root/dataset/converted_labelme_json/\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from shutil import copyfile\n",
    "\n",
    "# 输入输出路径\n",
    "input_json = '/root/project/research/Yolo/yoloe/custom_train.json'\n",
    "output_dir = '/root/dataset/converted_labelme_json/'\n",
    "\n",
    "# 读取原始的目标检测标注 JSON 文件\n",
    "with open(input_json, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 检查输出目录是否存在\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# 创建类别映射，将 category_id 映射到 category_name\n",
    "categories = {category['id']: category['name'] for category in data['categories']}\n",
    "\n",
    "# 假设目标检测 JSON 文件包含一个图片列表，每个图片有 'file_name' 和 'annotations' 字段\n",
    "for img_info in data['images']:\n",
    "    img_name = img_info['file_name']\n",
    "    img_id = img_info['id']\n",
    "    \n",
    "    # 查找该图片对应的标注\n",
    "    annotations = [ann for ann in data['annotations'] if ann['image_id'] == img_id]\n",
    "\n",
    "    # 创建 LabelMe 格式的 JSON\n",
    "    labelme_json = {\n",
    "        \"version\": \"4.5.7\",  # 这里可以根据实际版本修改\n",
    "        \"flags\": {},\n",
    "        \"shapes\": [],\n",
    "        \"imagePath\": img_name,\n",
    "        \"imageData\": None,\n",
    "        \"imageHeight\": img_info['height'],\n",
    "        \"imageWidth\": img_info['width']\n",
    "    }\n",
    "\n",
    "    # 转换目标检测框为 LabelMe 支持的格式\n",
    "    for ann in annotations:\n",
    "        xmin, ymin, width, height = ann['bbox']\n",
    "        xmax = xmin + width\n",
    "        ymax = ymin + height\n",
    "        \n",
    "        # 使用类别名称代替 category_id\n",
    "        category_name = categories.get(ann['category_id'], str(ann['category_id']))\n",
    "        \n",
    "        labelme_json['shapes'].append({\n",
    "            \"label\": category_name,  # 使用类别名称\n",
    "            \"points\": [\n",
    "                [xmin, ymin],  # 左上角\n",
    "                [xmax, ymax]   # 右下角\n",
    "            ],\n",
    "            \"group_id\": None,\n",
    "            \"description\": \"\",\n",
    "            \"shape_type\": \"rectangle\",  # 矩形框\n",
    "            \"flags\": {},\n",
    "            \"mask\": None  # 目标检测中没有掩膜\n",
    "        })\n",
    "\n",
    "        \"\"\" 这样就可以把目标检测的数据集拿去做分割，不会有sam2的误分割\n",
    "        labelme_json['shapes'].append({\n",
    "            \"label\": category_name,  # 使用类别名称，而不是类别ID\n",
    "            \"points\": [\n",
    "                [xmin, ymin],\n",
    "                [xmax, ymin],\n",
    "                [xmax, ymax],\n",
    "                [xmin, ymax]\n",
    "            ],\n",
    "            \"shape_type\": \"polygon\",  # 如果你使用的是矩形框，可以改成 \"rectangle\"\n",
    "            \"flags\": {}\n",
    "        })  \n",
    "        \"\"\"\n",
    "\n",
    "    # 保存为单个图片的 JSON 文件\n",
    "    output_json_path = os.path.join(output_dir, f'{img_name.split(\".\")[0]}.json')\n",
    "    with open(output_json_path, 'w') as f_out:\n",
    "        json.dump(labelme_json, f_out, indent=4)\n",
    "\n",
    "print(f\"转换完成，所有文件已保存到：{output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "85c4c044-eace-43b5-a4f1-1b9cf2a8c7fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "转换完成，所有文件已保存到：/root/dataset/converted_labelme_seg_json/\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from pycocotools import mask\n",
    "from shutil import copyfile\n",
    "\n",
    "# 输入输出路径\n",
    "input_json = '/root/project/research/Yolo/yoloe/custom_train_segm.json'\n",
    "output_dir = '/root/dataset/converted_labelme_seg_json/'\n",
    "\n",
    "# 读取 COCO 格式的标注 JSON 文件\n",
    "with open(input_json, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 检查输出目录是否存在\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# 创建类别映射，将 category_id 映射到 category_name\n",
    "categories = {category['id']: category['name'] for category in data['categories']}\n",
    "\n",
    "# COCO 格式中的 'images' 和 'annotations' 数据\n",
    "images = data['images']\n",
    "annotations = data['annotations']\n",
    "\n",
    "# 为每张图片生成对应的 LabelMe JSON 文件\n",
    "for img_info in images:\n",
    "    img_name = img_info['file_name']\n",
    "    img_id = img_info['id']\n",
    "    \n",
    "    # 查找该图片的分割标注\n",
    "    img_annotations = [ann for ann in annotations if ann['image_id'] == img_id]\n",
    "\n",
    "    # 创建 LabelMe 格式的 JSON\n",
    "    labelme_json = {\n",
    "        \"version\": \"4.5.7\",  # 可以根据实际版本修改\n",
    "        \"flags\": {},\n",
    "        \"shapes\": [],\n",
    "        \"imagePath\": img_name,\n",
    "        \"imageData\": None,\n",
    "        \"imageHeight\": img_info['height'],\n",
    "        \"imageWidth\": img_info['width']\n",
    "    }\n",
    "\n",
    "    # 转换 COCO 格式的分割为 LabelMe 支持的多边形格式\n",
    "    for ann in img_annotations:\n",
    "        # COCO 格式中的 'segmentation' 是一个多边形或 RLE\n",
    "        segmentation = ann['segmentation']\n",
    "        \n",
    "        if isinstance(segmentation, list):  # 多边形分割\n",
    "            for poly in segmentation:\n",
    "                # 使用类别名称代替 category_id\n",
    "                category_name = categories.get(ann['category_id'], str(ann['category_id']))\n",
    "                labelme_json['shapes'].append({\n",
    "                    \"label\": category_name,  # 使用类别名称\n",
    "                    \"points\": np.array(poly).reshape(-1, 2).tolist(),\n",
    "                    \"shape_type\": \"polygon\",  # 如果你想使用矩形，改成 \"rectangle\"\n",
    "                    \"flags\": {}\n",
    "                })\n",
    "        elif isinstance(segmentation, dict):  # RLE 分割\n",
    "            rle = segmentation\n",
    "            # 解码 RLE\n",
    "            mask_array = mask.decode(rle)\n",
    "            # 获取分割区域的轮廓点\n",
    "            contours = find_contours(mask_array)\n",
    "            for contour in contours:\n",
    "                # 使用类别名称代替 category_id\n",
    "                category_name = categories.get(ann['category_id'], str(ann['category_id']))\n",
    "                labelme_json['shapes'].append({\n",
    "                    \"label\": category_name,  # 使用类别名称\n",
    "                    \"points\": contour.tolist(),\n",
    "                    \"shape_type\": \"polygon\",\n",
    "                    \"flags\": {}\n",
    "                })\n",
    "\n",
    "    # 保存为单个图片的 JSON 文件\n",
    "    output_json_path = os.path.join(output_dir, f'{img_name.split(\".\")[0]}.json')\n",
    "    with open(output_json_path, 'w') as f_out:\n",
    "        json.dump(labelme_json, f_out, indent=4)\n",
    "\n",
    "print(f\"转换完成，所有文件已保存到：{output_dir}\")\n",
    "\n",
    "def find_contours(mask_array):\n",
    "    \"\"\"\n",
    "    使用 OpenCV 查找二值化掩膜中的轮廓。\n",
    "    \"\"\"\n",
    "    import cv2\n",
    "    contours, _ = cv2.findContours(mask_array.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    return [contour.reshape(-1, 2) for contour in contours]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19a287b-ace0-47f0-8a4d-e3649ce9148b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
